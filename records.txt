# Personal records of game score
22-12-17: 190

# Linking with AI
Measurements for 30 iterations of 0th generation
Note that each iteration had a typical score of 8 to 12.
    Uncapping FPS:
        147.70 s -> 19.96 s (7.4x improvement)
    Headless:
        19.96 s -> 0.83 s

    Assuming that a typical run when the network has been trained takes 10x longer,
    And that performance scales perfectly with 32 cores,
    It takes 0.83 * 10 / 30 / 32 == 0.0086 seconds per iteration.

    Let's run each Breaker 5 times, for example, so that the middle 3 values can be averaged for the fitness.

    If we have n Breakers in each generation,
    we run 5 * n iterations total, giving 0.0086 * 5 * n == 0.043 * n.
    Let's just say (0.05 * n) seconds for each generation

    If we want to limit each generation to about 1 minute,
    we get n == 1200.
    I don't think that a larger population size will be critical in the end result,
    so let's have n == 1024 to make things nice.

Testing scaling with multiprocessing
    1 process, n=1024 at gen 0 took 35 seconds.
    32 processes, n=1024 at gen 0 took 2.9 seconds.
    --> This is a 12x improvement. Hmm..

    1 process, n=4096 at gen 0 took 139 seconds.
    16 processes, n=4096 at gen 0 took 15.3 seconds.
    32 processes, n=4096 at gen 0 took 11.4 seconds.
    --> Still a 12x improvement. I guess this is what the truth is.

---

# New calculations for bootstrapping-based AI

From the previous AI attempt using GA, we have:
    AVG score 70, 5120 iterations --> 230 seconds
    230 / 5120 == 45 ms / iteration
    45 / 70 == 0.64 ms / round

We can estimate that a single firing round takes about 0.6 ms on 32 cores.
That translates to about 8 ms on one core. (assuming x12 scaling as observed earlier)

If we divide the angle of fire into 1000 incremental steps,
It will take 8 seconds to simulate all of them.
Yikes.

Good thing we have 32 cores, eh?
Back to 0.6 ms per round.
That yields 0.6 seconds for simulating each round.
Assume length of 100 rounds per game.
That yields 60 seconds per game. (This is even ignoring the evaluation overhead)

To acquire 10,000 samples, we need 100 minutes.
To acquire 100,000 samples, we need 1000 minutes, i.e. 17 hours.

...I think a speedup is necessary.
Why don't we try preprocess the collision detections?

Position can be in increments of 0.1 pixels, and the angle in 21600 steps(1 arcmin each).
There are 6 possible wall configurations.
((120 / 2) / 0.1) * ((75 / 2) / 0.1) * 21600 * 6 == 29,160,000,000
If we do a 10 million calculations per second, this will take less than an hour to preprocess.

Will the speedup be significant?
I think the quickest way to find out... is to just give it try.
It isn't even hard to implement.

I just tried timing how long the collision handling logic takes.
It was responsible for 15 seconds out of the 230 for each iteration of GA.
No point in the optimization mentioned earlier.
Abort!

# Performance fix

Just found out that there was a lot of redundant AI calculations going on,
because the AI was being run even when the game wasn't in its responsive state.

Resolving this brought the GA runtime from 230 seconds down to 168 seconds.
This is a 27% gain in speed. Not bad.

TODO: Create explorer-evaluator loop in bootstrapper. Perhaps start by running explorer only.