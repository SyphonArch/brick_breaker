# Personal records of game score
22-12-17: 190

# Linking with AI
Measurements for 30 iterations of 0th generation
Note that each iteration had a typical score of 8 to 12.
    Uncapping FPS:
        147.70 s -> 19.96 s (7.4x improvement)
    Headless:
        19.96 s -> 0.83 s

    Assuming that a typical run when the network has been trained takes 10x longer,
    And that performance scales perfectly with 32 cores,
    It takes 0.83 * 10 / 30 / 32 == 0.0086 seconds per iteration.

    Let's run each Breaker 5 times, for example, so that the middle 3 values can be averaged for the fitness.

    If we have n Breakers in each generation,
    we run 5 * n iterations total, giving 0.0086 * 5 * n == 0.043 * n.
    Let's just say (0.05 * n) seconds for each generation

    If we want to limit each generation to about 1 minute,
    we get n == 1200.
    I don't think that a larger population size will be critical in the end result,
    so let's have n == 1024 to make things nice.

Testing scaling with multiprocessing
    1 process, n=1024 at gen 0 took 35 seconds.
    32 processes, n=1024 at gen 0 took 2.9 seconds.
    --> This is a 12x improvement. Hmm..

    1 process, n=4096 at gen 0 took 139 seconds.
    16 processes, n=4096 at gen 0 took 15.3 seconds.
    32 processes, n=4096 at gen 0 took 11.4 seconds.
    --> Still a 12x improvement. I guess this is what the truth is.

---

# New calculations for bootstrapping-based AI

From the previous AI attempt using GA, we have:
    AVG score 70, 5120 iterations --> 220 seconds
    220 / 5120 == 43 ms / iteration
    43 / 70 == 0.6 ms / iteration

We can estimate that a single firing round takes about 0.6 ms on 32 cores.
That translates to about 20 ms on one core.

If we divide the angle of fire into 1000 incremental steps,
It will take 20 seconds to simulate all of them.
Yikes.

Good thing we have 32 cores, eh?
Back to 0.6 ms per round.
That yields 0.6 seconds for simulating each round.
Assume length of 100 rounds per game.
That yields 60 seconds per game. (This is even ignoring the evaluation overhead)

To acquire 10,000 samples, we need 100 minutes.
To acquire 100,000 samples, we need 1000 minutes, i.e. 17 hours.

...I think a speedup is necessary.
Why don't we try preprocess the collision detections?

Position can be in increments of 0.1 pixels, and the angle in 21600 steps(1 arcmin each).
There are 6 possible wall configurations.
((120 / 2) / 0.1) * ((75 / 2) / 0.1) * 21600 * 6 == 29,160,000,000
If we do a 10 million calculations per second, this will take less than an hour to preprocess.

Will the speedup be significant?
I think the quickest way to find out... is to just give it try.
It isn't even hard to implement.
